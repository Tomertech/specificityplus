# Uncovering Failures of Model Editing in Large Language Models: An Improved Specificity Benchmark

This repository contains the code for the paper [Uncovering Limits of Memory Editing in Large Language Models: A New Specificity Benchmark]() (ACL Findings 2023). # todo: add link

It extends previous work on model editing by Meng et al. #todo: add citations by introducing a new benchmark, called CounterFact+, for measuring the specificity of model edits. 

## Attribution and Installation
The repository is a fork of https://github.com/kmeng01/memit, which implement the model editing algorithms MEMIT (Mass Editing Memory in a Transformer) and ROME (Rank-One Model Editing). Our fork extends this code by additional evaluation scripts implementing the CounterFact+ benchmark. For installation instructions see the original repository.

## How to Cite
#todo: update citation 
```bibtex
@article{hoelscher2023specificityplus,
  title={Uncovering Failures of Model Editing in Large Language Models: An Improved Specificity Benchmark},
  author={Jason Hoelscher-Obermaier and Persson, Julia, and Kran, Esben and Konstas, Ioannis and Barez, Fazl},
  journal={}, # todo: add link
  year={2023}
}
```
